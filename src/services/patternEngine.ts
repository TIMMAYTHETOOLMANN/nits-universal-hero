import { CustomPattern, PatternTestResults } from '../types/patterns'
import { AnalysisResult } from '../types/analysis'
import { DEFAULT_PATTERN_CONFIDENCE, DEFAULT_PATTERN_SEVERITY } from '../constants/analysisConfig'
import { formatTimestamp } from '../utils/dateUtils'

declare global {
  interface Window {
    spark: {
      llmPrompt: (strings: TemplateStringsArray, ...values: any[]) => string
      llm: (prompt: string, modelName?: string, jsonMode?: boolean) => Promise<string>
    }
  }
}

const spark = window.spark

export class PatternEngineError extends Error {
  constructor(message: string, public cause?: Error) {
    super(message)
    this.name = 'PatternEngineError'
  }
}

export class PatternEngine {
  private static instance: PatternEngine

  static getInstance(): PatternEngine {
    if (!PatternEngine.instance) {
      PatternEngine.instance = new PatternEngine()
    }
    return PatternEngine.instance
  }

  async testPattern(
    pattern: CustomPattern,
    onLog: (message: string) => void
  ): Promise<PatternTestResults> {
    onLog(`Testing pattern: ${pattern.name}`)
    
    try {
      const testPrompt = spark.llmPrompt`
        Test this custom forensic pattern for effectiveness:

        Pattern: ${pattern.name}
        Description: ${pattern.description}
        Category: ${pattern.category}
        Keywords: ${pattern.keywords.join(', ')}
        Rules: ${pattern.rules.join(' | ')}
        Severity: ${pattern.severity}
        Current Confidence: ${pattern.confidence}

        Simulate testing this pattern against various document types and scenarios.
        Evaluate:
        1. Detection accuracy
        2. False positive rate
        3. Coverage completeness
        4. Rule precision

        Return JSON:
        {
          "totalTests": 50,
          "successRate": 0.85,
          "falsePositives": 7,
          "improvements": ["improvement1", "improvement2"],
          "effectiveness": "high|medium|low"
        }
      `

      const result = await spark.llm(testPrompt, 'gpt-4o', true)
      const testData = JSON.parse(result)
      
      const testResults: PatternTestResults = {
        totalTests: testData.totalTests || 50,
        successRate: Math.min(95, testData.successRate || 75),
        falsePositives: testData.falsePositives || 5,
        lastTestDate: formatTimestamp()
      }

      onLog(`Pattern test complete: ${testResults.successRate}% success rate, ${testResults.falsePositives} false positives`)
      return testResults
      
    } catch (error) {
      onLog(`Pattern test failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
      return {
        totalTests: 10,
        successRate: 60,
        falsePositives: 4,
        lastTestDate: formatTimestamp()
      }
    }
  }

  async generateAutonomousPatterns(
    analysisResults: AnalysisResult,
    existingPatterns: CustomPattern[],
    onLog: (message: string) => void
  ): Promise<CustomPattern[]> {
    try {
      onLog('Analyzing results for autonomous pattern generation...')
      
      // Get existing pattern context for improvement (not replacement)
      const existingPatternContext = existingPatterns
        .filter(p => p.name.startsWith('[AUTO]'))
        .map(p => `${p.name}: ${p.description} (Success Rate: ${p.testResults.successRate}%)`)
        .join('\n')
      
      const patternPrompt = spark.llmPrompt`
        Based on this forensic analysis, generate NEW custom patterns that would ENHANCE detection accuracy beyond existing patterns:

        Analysis Results:
        - Risk Score: ${analysisResults.summary.riskScore}/10
        - Anomalies Found: ${analysisResults.anomalies.length}
        - Key Anomalies: ${analysisResults.anomalies.map(a => `${a.type}: ${a.description}`).join('; ')}
        - NLP Insights: ${JSON.stringify(analysisResults.nlpSummary)}
        - Total Custom Patterns: ${existingPatterns.length}
        - Active Custom Patterns: ${existingPatterns.filter(p => p.isActive).length}
        
        Existing Auto-Generated Patterns (DO NOT DUPLICATE):
        ${existingPatternContext || 'None yet created'}
        
        REQUIREMENTS:
        1. Generate 2-3 NEW patterns that are DIFFERENT from existing ones
        2. Focus on gaps not covered by current patterns
        3. Each new pattern should target specific anomalies found in this analysis
        4. Patterns should be CUMULATIVE - adding to existing detection capabilities
        5. Higher sophistication based on analysis complexity
        
        Return JSON with this structure:
        {
          "patterns": [
            {
              "name": "pattern name (must be unique from existing)",
              "description": "detailed description targeting specific gap in current detection",
              "category": "insider-trading|esg-greenwashing|financial-engineering|disclosure-gap|litigation-risk|temporal-anomaly|custom",
              "keywords": ["keyword1", "keyword2", "keyword3"],
              "rules": ["rule1", "rule2"],
              "severity": "low|medium|high|critical",
              "confidence": 0.7,
              "reasoning": "why this pattern fills a gap in current detection capabilities"
            }
          ],
          "trainingStrategy": "cumulative enhancement strategy",
          "expectedImprovements": ["improvement1", "improvement2"],
          "gapsFilled": ["gap1", "gap2"]
        }
      `

      const patternResult = await spark.llm(patternPrompt, 'gpt-4o', true)
      const patternData = JSON.parse(patternResult)
      
      const newPatterns: CustomPattern[] = patternData.patterns.map((p: {
        name: string
        description: string
        category: string
        keywords: string[]
        rules: string[]
        severity: string
        confidence: number
        reasoning: string
      }, index: number) => ({
        id: `auto_pattern_${Date.now()}_${index}_${Math.random().toString(36).substr(2, 9)}`,
        name: `[AUTO-${existingPatterns.filter(cp => cp.name.startsWith('[AUTO]')).length + index + 1}] ${p.name}`,
        description: `${p.description}\n\nðŸ¤– AUTO-GENERATED: ${p.reasoning}\n\nGenerated from analysis with ${analysisResults.summary.riskScore.toFixed(1)}/10 risk score and ${analysisResults.anomalies.length} anomalies detected.`,
        category: (p.category as any) || 'custom',
        keywords: p.keywords || [],
        rules: p.rules || [],
        severity: (p.severity as any) || DEFAULT_PATTERN_SEVERITY,
        confidence: Math.min(0.95, (p.confidence || DEFAULT_PATTERN_CONFIDENCE) + (analysisResults.summary.riskScore / 20)), // Improve confidence based on risk score
        isActive: true,
        createdAt: formatTimestamp(),
        lastTested: null,
        testResults: {
          totalTests: 0,
          successRate: 0,
          falsePositives: 0,
          lastTestDate: null
        }
      }))

      onLog(`Generated ${newPatterns.length} NEW autonomous patterns to enhance existing ${existingPatterns.filter(p => p.name.startsWith('[AUTO]')).length} auto-patterns`)
      return newPatterns
      
    } catch (error) {
      onLog('Failed to generate autonomous patterns - maintaining existing capabilities')
      return [] // Don't create fallback patterns that might conflict
    }
  }

  async optimizeExistingPatterns(
    patterns: CustomPattern[],
    onLog: (message: string) => void
  ): Promise<CustomPattern[]> {
    const testablePatterns = patterns.filter(p => p.testResults.totalTests > 0)
    if (testablePatterns.length === 0) {
      onLog('No patterns with test history available for optimization')
      return patterns
    }

    onLog(`Optimizing ${testablePatterns.length} existing patterns based on performance data...`)

    const optimizedPatterns = [...patterns]
    
    for (const pattern of testablePatterns) {
      try {
        if (pattern.testResults.successRate < 70) {
          // Pattern needs improvement
          const optimizationPrompt = spark.llmPrompt`
            Optimize this underperforming forensic pattern:

            Current Pattern:
            - Name: ${pattern.name}
            - Description: ${pattern.description}
            - Keywords: ${pattern.keywords.join(', ')}
            - Rules: ${pattern.rules.join(' | ')}
            - Success Rate: ${pattern.testResults.successRate}%
            - False Positives: ${pattern.testResults.falsePositives}

            Suggest improvements to increase accuracy and reduce false positives.
            Return JSON:
            {
              "optimizedKeywords": ["improved", "keywords"],
              "optimizedRules": ["improved rule1", "improved rule2"],
              "reasoning": "explanation of improvements"
            }
          `

          const optimizationResult = await spark.llm(optimizationPrompt, 'gpt-4o', true)
          const optimization = JSON.parse(optimizationResult)
          
          // Apply optimizations
          const patternIndex = optimizedPatterns.findIndex(p => p.id === pattern.id)
          if (patternIndex !== -1) {
            optimizedPatterns[patternIndex] = {
              ...optimizedPatterns[patternIndex],
              keywords: optimization.optimizedKeywords || pattern.keywords,
              rules: optimization.optimizedRules || pattern.rules,
              description: `${pattern.description}\n\nðŸ”§ OPTIMIZED: ${optimization.reasoning || 'Performance improvements applied'}`
            }
            onLog(`Optimized pattern: ${pattern.name}`)
          }
        }
      } catch (error) {
        onLog(`Failed to optimize pattern: ${pattern.name}`)
      }
    }

    return optimizedPatterns
  }

  validatePattern(pattern: Partial<CustomPattern>): { isValid: boolean, errors: string[] } {
    const errors: string[] = []

    if (!pattern.name || pattern.name.trim().length === 0) {
      errors.push('Pattern name is required')
    }

    if (!pattern.description || pattern.description.trim().length === 0) {
      errors.push('Pattern description is required')
    }

    if (!pattern.keywords || pattern.keywords.length === 0) {
      errors.push('At least one keyword is required')
    }

    if (!pattern.rules || pattern.rules.length === 0) {
      errors.push('At least one rule is required')
    }

    if (pattern.confidence !== undefined && (pattern.confidence < 0.1 || pattern.confidence > 1.0)) {
      errors.push('Confidence must be between 0.1 and 1.0')
    }

    return {
      isValid: errors.length === 0,
      errors
    }
  }

  createPattern(patternData: Partial<CustomPattern>): CustomPattern {
    const validation = this.validatePattern(patternData)
    if (!validation.isValid) {
      throw new PatternEngineError(`Invalid pattern data: ${validation.errors.join(', ')}`)
    }

    return {
      id: `pattern_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      name: patternData.name!,
      description: patternData.description!,
      category: patternData.category || 'custom',
      keywords: patternData.keywords || [],
      rules: patternData.rules || [],
      severity: patternData.severity || DEFAULT_PATTERN_SEVERITY,
      confidence: patternData.confidence || DEFAULT_PATTERN_CONFIDENCE,
      isActive: patternData.isActive !== undefined ? patternData.isActive : true,
      createdAt: formatTimestamp(),
      lastTested: null,
      testResults: {
        totalTests: 0,
        successRate: 0,
        falsePositives: 0,
        lastTestDate: null
      }
    }
  }

  togglePatternActive(patterns: CustomPattern[], patternId: string): CustomPattern[] {
    return patterns.map(pattern => 
      pattern.id === patternId 
        ? { ...pattern, isActive: !pattern.isActive }
        : pattern
    )
  }

  deletePattern(patterns: CustomPattern[], patternId: string): CustomPattern[] {
    return patterns.filter(pattern => pattern.id !== patternId)
  }

  updatePattern(patterns: CustomPattern[], patternId: string, updates: Partial<CustomPattern>): CustomPattern[] {
    return patterns.map(pattern => 
      pattern.id === patternId 
        ? { ...pattern, ...updates }
        : pattern
    )
  }
}